{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usami/miniconda3/envs/human_action/lib/python3.8/site-packages/scapy/layers/ipsec.py:469: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  cipher=algorithms.Blowfish,\n",
      "/home/usami/miniconda3/envs/human_action/lib/python3.8/site-packages/scapy/layers/ipsec.py:483: CryptographyDeprecationWarning: CAST5 has been deprecated and will be removed in a future release\n",
      "  cipher=algorithms.CAST5,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "from scapy.all import rdpcap, IP, TCP, UDP, ICMP\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "if os.path.isfile('/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-nor.csv'):\n",
    "    nor = {}\n",
    "    ddos = {}\n",
    "    port = {}\n",
    "    bot = {}\n",
    "    nor['ID'] = pd.read_csv('/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-nor.csv')\n",
    "    ddos['ID'] = pd.read_csv('/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-ddos.csv')\n",
    "    port['ID'] = pd.read_csv('/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-port.csv')\n",
    "    bot['ID'] = pd.read_csv('/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-bot.csv')\n",
    "else:\n",
    "    file_path_dos = '/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "    file_path_port = '/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv'\n",
    "    file_path_bot = '/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-Morning.pcap_ISCX.csv'\n",
    "    df_ddos = pd.read_csv(file_path_dos)\n",
    "    df_port = pd.read_csv(file_path_port)\n",
    "    df_bot = pd.read_csv(file_path_bot)\n",
    "    saddr = ' Source IP'\n",
    "    sport = ' Source Port'\n",
    "    daddr = ' Destination IP'\n",
    "    dport = ' Destination Port'\n",
    "    proto = ' Protocol'\n",
    "\n",
    "    label = ' Label'\n",
    "    nor_label = 'BENIGN'\n",
    "    bot_label = 'Bot'\n",
    "    port_label = 'PortScan'\n",
    "    ddos_label = 'DDoS'\n",
    "\n",
    "    df_ddos = df_ddos[df_ddos[proto] != 0]\n",
    "    df_port = df_port[df_port[proto] != 0]\n",
    "    df_bot = df_bot[df_bot[proto] != 0]\n",
    "\n",
    "    def create_id(row):\n",
    "        if row[proto] in [6, 17]:  # TCP or UDP\n",
    "            id_str = f\"{row[saddr]}:{int(row[sport])}->{row[daddr]}:{int(row[dport])}:{row[proto]}\"\n",
    "        elif row[proto] == 1:  # ICMP\n",
    "            id_str = f\"{row[saddr]}->{row[daddr]}:1\"\n",
    "        return id_str\n",
    "        # return int(hashlib.md5(id_str.encode()).hexdigest(), 32)\n",
    "    nor = df_ddos[df_ddos[label] == nor_label]\n",
    "    pd.concat([nor, df_port[df_port[label] == nor_label]], ignore_index=True)\n",
    "    pd.concat([nor, df_bot[df_bot[label] == nor_label]], ignore_index=True)\n",
    "\n",
    "    ddos = df_ddos[df_ddos[label] == ddos_label]\n",
    "    port = df_port[df_port[label] == port_label]\n",
    "    bot = df_bot[df_bot[label] == bot_label]\n",
    "\n",
    "    nor['ID'] = nor.apply(create_id, axis=1)\n",
    "    ddos['ID'] = ddos.apply(create_id, axis=1)\n",
    "    port['ID'] = port.apply(create_id, axis=1)\n",
    "    bot['ID'] = bot.apply(create_id, axis=1)\n",
    "    duplicate_ddos = pd.Series(list(set(nor['ID']).intersection(set(ddos['ID']))))\n",
    "    duplicate_port = pd.Series(list(set(nor['ID']).intersection(set(port['ID']))))\n",
    "    duplicate_bot = pd.Series(list(set(nor['ID']).intersection(set(bot['ID']))))\n",
    "\n",
    "    nor = nor[~nor['ID'].isin(duplicate_ddos)]\n",
    "    nor = nor[~nor['ID'].isin(duplicate_port)]\n",
    "    nor = nor[~nor['ID'].isin(duplicate_bot)]\n",
    "    nor_id = pd.DataFrame(nor['ID'].unique())\n",
    "    ddos_id = pd.DataFrame(ddos['ID'].unique())\n",
    "    port_id = pd.DataFrame(port['ID'].unique())\n",
    "    bot_id = pd.DataFrame(bot['ID'].unique())\n",
    "    nor_id.to_csv('/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-nor.csv', index=False)\n",
    "    ddos_id.to_csv('/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-ddos.csv', index=False)\n",
    "    port_id.to_csv('/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-port.csv', index=False)\n",
    "    bot_id.to_csv('/home/usami/TBN64/dataset_pre/CICDDoS2017/TrafficLabelling/Friday-WorkingHours-bot.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = '/home/usami/TBN64/dataset_pre/CICDDoS2017/Friday-WorkingHours'\n",
    "TIMEOUT = 30\n",
    "cnt_normal = 0\n",
    "\n",
    "def read_pcap_with_tshark(file_path):\n",
    "    command = [\n",
    "        'tshark', '-r', file_path, '-T', 'fields',\n",
    "        '-e', 'frame.time_epoch',\n",
    "        '-e', 'ip.src', '-e', 'ip.dst',\n",
    "        '-e', 'ip.proto',\n",
    "        '-e', 'tcp.srcport', '-e', 'tcp.dstport', '-e', 'tcp.flags',\n",
    "        '-e', 'udp.srcport', '-e', 'udp.dstport',\n",
    "        '-e', 'icmp.type',\n",
    "        '-e', 'frame.len', '-e', 'ip.flags', '-e', 'ip.ttl'\n",
    "    ]\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    for line in process.stdout:\n",
    "        yield line.decode().strip().split('\\t')\n",
    "\n",
    "for filename in range(0, 50):\n",
    "    file_path = folder_path + '/Friday__000' + str(filename).zfill(2) + '.pcap'\n",
    "    cnt_normal = 0\n",
    "    if os.path.isfile(file_path):\n",
    "        print(file_path)\n",
    "        flows = {}\n",
    "        flows_cnt = {}\n",
    "\n",
    "        for packet in tqdm(read_pcap_with_tshark(file_path)):\n",
    "            if len(packet) < 12:\n",
    "                continue\n",
    "            \n",
    "            timestamp, src_ip, dst_ip, protocol_numbers, tcp_sport, tcp_dport, tcp_flags, udp_sport, udp_dport, icmp_type, pkt_len, ip_flags, ttl = packet\n",
    "            proto_layer = None\n",
    "            flow_id_str = None\n",
    "            \n",
    "            for protocol_number in protocol_numbers.split(','):\n",
    "                protocol_number = int(protocol_number)\n",
    "                \n",
    "                if tcp_sport and tcp_dport:\n",
    "                    proto_layer = 'TCP'\n",
    "                    flow_id_str = f\"{src_ip}:{tcp_sport}->{dst_ip}:{tcp_dport}:{protocol_number}\"\n",
    "                elif udp_sport and udp_dport:\n",
    "                    proto_layer = 'UDP'\n",
    "                    flow_id_str = f\"{src_ip}:{udp_sport}->{dst_ip}:{udp_dport}:{protocol_number}\"\n",
    "                elif icmp_type:\n",
    "                    proto_layer = 'ICMP'\n",
    "                    flow_id_str = f\"{src_ip}->{dst_ip}:{protocol_number}\"\n",
    "                \n",
    "                if flow_id_str:\n",
    "                    if nor['ID'].isin([flow_id_str]).sum().values or ddos['ID'].isin([flow_id_str]).sum().values:\n",
    "                        flow_id_hash = int(hashlib.md5(flow_id_str.encode()).hexdigest(), 32)\n",
    "                        if flow_id_hash not in flows_cnt:\n",
    "                            flows_cnt[flow_id_hash] = 0\n",
    "                        if proto_layer == 'TCP':\n",
    "                            if int(tcp_flags, 16) & 0x02 and not int(tcp_flags, 16) & 0x10:  # SYN flag set and ACK flag not set\n",
    "                                flows_cnt[flow_id_hash] += 1\n",
    "                        flow_id = int(hashlib.md5((flow_id_str + ':' + str(flows_cnt[flow_id_hash])).encode()).hexdigest(), 32)\n",
    "                        if flow_id in flows:\n",
    "                            last_packet_time = flows[flow_id][-1]['timestamp']\n",
    "                            if (float(timestamp) - last_packet_time) > TIMEOUT:\n",
    "                                flows_cnt[flow_id_hash] += 1\n",
    "                                flow_id = int(hashlib.md5((flow_id_str + ':' + str(flows_cnt[flow_id_hash])).encode()).hexdigest(), 32)\n",
    "\n",
    "                        if flow_id:\n",
    "                            sorted_flow_id = flow_id\n",
    "                            if sorted_flow_id not in flows:\n",
    "                                flows[sorted_flow_id] = []\n",
    "                            packet_info = {\n",
    "                                'id': flow_id_str,\n",
    "                                'timestamp': float(timestamp),\n",
    "                                'time': 0,\n",
    "                                'proto': protocol_number,\n",
    "                                'pkt_len': int(pkt_len),\n",
    "                                'ip_flag': int(ip_flags, 16) if ip_flags else 0,\n",
    "                                'ttl': int(ttl, 16) if ttl else 0,\n",
    "                                'tcp_len': None,\n",
    "                                'tcp_ack': None,\n",
    "                                'tcp_flag': int(tcp_flags, 16) if tcp_flags else 0,\n",
    "                                'udp_len': None,\n",
    "                                'icmp_type': int(icmp_type, 16) if icmp_type else 0,\n",
    "                                'label': 2  # 0: Normal, 1: DoS, 2: DDoS\n",
    "                            }\n",
    "\n",
    "                            if proto_layer == 'TCP':\n",
    "                                packet_info['tcp_len'] = int(tcp_sport, 16)\n",
    "                            elif proto_layer == 'UDP':\n",
    "                                packet_info['udp_len'] = int(udp_sport, 16)\n",
    "                            \n",
    "                            if flows[sorted_flow_id]:\n",
    "                                first_packet_time = flows[sorted_flow_id][0]['timestamp']\n",
    "                                packet_info['time'] = float(timestamp) - first_packet_time\n",
    "                            if packet_info['time'] == 0:\n",
    "                                if nor['ID'].isin([flow_id_str]).sum().values:\n",
    "                                    packet_info['label'] = 0\n",
    "                                    cnt_normal += 1\n",
    "                            else:\n",
    "                                packet_info['label'] = flows[sorted_flow_id][0]['label']\n",
    "                            flows[sorted_flow_id].append(packet_info)\n",
    "\n",
    "        flow_arrays = list(flows.values())\n",
    "        for flow_id in flows:\n",
    "            for packet in flows[flow_id]:\n",
    "                del packet['timestamp']\n",
    "        print(f\"Số lượng các luồng Normal: {cnt_normal}\")\n",
    "        print(f\"Số lượng các luồng: {len(flow_arrays)}\")\n",
    "        folroot = os.path.dirname(folder_path)\n",
    "        for folname in [folroot+'/data',\n",
    "                        folroot+'/label']:\n",
    "            if not os.path.exists(folname):\n",
    "                    os.makedirs(folname)\n",
    "        output_file_path = os.path.dirname(os.path.dirname(file_path))+'/data/'+file_path.split('/')[-1].replace('.pcap','.json')\n",
    "        data = []\n",
    "        lb = []\n",
    "        time_window = 60\n",
    "        h_window = 32\n",
    "        e = time_window / h_window\n",
    "        \n",
    "        for flow_id in flows:\n",
    "            data_array = []\n",
    "            flow = flows[flow_id]\n",
    "            for h in range(h_window):\n",
    "                data_array_t = []\n",
    "                for record in flow:\n",
    "                    if (record['time'] < (h + 1) * e) and (record['time'] >= h * e):\n",
    "                        record_list = [\n",
    "                            record['proto'],\n",
    "                            record['pkt_len'],\n",
    "                            record['ip_flag'],\n",
    "                            record['ttl'],\n",
    "                            record['tcp_len'] if record['tcp_len'] is not None else 0,\n",
    "                            record['tcp_ack'] if record['tcp_ack'] is not None else 0,\n",
    "                            record['tcp_flag'] if record['tcp_flag'] is not None else 0,\n",
    "                            record['udp_len'] if record['udp_len'] is not None else 0,\n",
    "                            record['icmp_type'] if record['icmp_type'] is not None else 0,\n",
    "                        ]\n",
    "                        data_array_t.append(record_list)\n",
    "                if len(data_array_t) == 0 or np.isnan(np.mean(data_array_t, axis=0)).any():\n",
    "                    data_array_t_mean = np.zeros((1, 9))\n",
    "                else:\n",
    "                    data_array_t_mean = np.mean(data_array_t, axis=0)\n",
    "                data_array.append(data_array_t_mean)\n",
    "            data.append(np.vstack(data_array))\n",
    "            lb.append(record['label'])\n",
    "        \n",
    "        np.save(output_file_path.replace('.json', '.npy'), data, allow_pickle=True)\n",
    "        np.save(output_file_path.replace('.json', 'labels.npy'), lb, allow_pickle=True)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py\n",
    "\n",
    "\n",
    "def unique_rows(matrix):\n",
    "    seen = set()\n",
    "    unique_indices = []\n",
    "    \n",
    "    for i, row in enumerate(matrix):\n",
    "        row_hash = hash(row.tobytes())\n",
    "        if row_hash not in seen:\n",
    "            seen.add(row_hash)\n",
    "            unique_indices.append(i)\n",
    "    unique_matrix = matrix[unique_indices]\n",
    "    return unique_matrix\n",
    "def round_to_n_significant_figures(tensor, n):\n",
    "    \"\"\"\n",
    "    Efficiently round each element in a NumPy array to n significant figures.\n",
    "    Handles edge cases to avoid NaN values.\n",
    "    \"\"\"\n",
    "    tensor = np.asarray(tensor)\n",
    "    # Handle zero separately to avoid division by zero\n",
    "    non_zero_mask = tensor != 0\n",
    "    abs_tensor = np.abs(tensor[non_zero_mask])\n",
    "    order_of_magnitude = np.floor(np.log10(abs_tensor))\n",
    "    scale = np.power(10, n - 1 - order_of_magnitude)\n",
    "    tensor[non_zero_mask] = np.round(tensor[non_zero_mask] * scale) / scale\n",
    "    return tensor\n",
    "for h_window in [64,32,16,8]:\n",
    "    datala_list = []\n",
    "    time_window = 1\n",
    "    folder_root = '/mnt/d/dataset_pre/CICDDoS2017'\n",
    "    folder_path = folder_root+f'/h{str(h_window)}b/time_window_{str(time_window)}s/data'\n",
    "    cnt = 0\n",
    "    cnt_file = 0\n",
    "    for i in range(1000):\n",
    "        # if i in range(0,50) or i in range(214,283):\n",
    "        if 1:\n",
    "            filename = str(i) + '.npy'\n",
    "            if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "                if(cnt%10==0 and cnt !=0):\n",
    "                    print(cnt)\n",
    "                if(cnt%100==0 and cnt !=0):\n",
    "                    datalas = np.vstack(datala_list)\n",
    "                    datalas = round_to_n_significant_figures(datalas,3) \n",
    "                    datala_t = unique_rows(datalas)\n",
    "                    data = datala_t[:, :-1].reshape(-1, h_window, 9).astype(np.float32)\n",
    "                    labels = datala_t[:, -1].astype(int)\n",
    "\n",
    "                    np.save(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_data.npy', data)\n",
    "                    np.save(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_labels.npy', labels)\n",
    "                    del datalas\n",
    "                    del datala_t\n",
    "                    del data\n",
    "                    del labels\n",
    "                    gc.collect()\n",
    "                    datala_list = []\n",
    "                    cnt_file+=1\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                data = np.load(file_path)\n",
    "                labels = np.load(file_path.replace('/data/', '/labels/'))\n",
    "                data = data.reshape(data.shape[0], -1).reshape(data.shape[0], -1)\n",
    "                labels = labels.reshape(-1, 1)\n",
    "\n",
    "                datala_e = np.hstack((data, labels))\n",
    "                datala_list.append(datala_e)\n",
    "                cnt+=1\n",
    "                del data\n",
    "                del labels\n",
    "                del datala_e\n",
    "                gc.collect()\n",
    "\n",
    "    datalas = np.vstack(datala_list)\n",
    "    datala_t = unique_rows(datalas)\n",
    "    data = datala_t[:, :-1].reshape(-1, h_window, 9).astype(np.float32)\n",
    "    labels = datala_t[:, -1].astype(int)\n",
    "\n",
    "    np.save(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_data.npy', data)\n",
    "    np.save(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_labels.npy', labels)\n",
    "    del data\n",
    "    del labels\n",
    "    del datala_t\n",
    "    del datalas\n",
    "    gc.collect()\n",
    "    datala_list = []\n",
    "    datala_list_end = []\n",
    "    tt = cnt_file+1\n",
    "    cnt_file = 0\n",
    "    for cnt_file in range(tt):\n",
    "        print(cnt_file)\n",
    "        if os.path.isfile(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_data.npy'):\n",
    "            data = np.load(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_data.npy')\n",
    "            labels = np.load(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_labels.npy')\n",
    "            os.remove(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_data.npy')\n",
    "            os.remove(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_labels.npy')\n",
    "\n",
    "            data = data.reshape(data.shape[0], -1).reshape(data.shape[0], -1)\n",
    "            labels = labels.reshape(-1, 1)\n",
    "            datala_e = np.hstack((data, labels))\n",
    "            datala_list.append(datala_e)\n",
    "            del data\n",
    "            del labels\n",
    "            del datala_e\n",
    "            gc.collect()\n",
    "\n",
    "    datalas = np.vstack(datala_list)\n",
    "    data = datalas[:, :-1].reshape(-1, h_window, 9).astype(np.float32)\n",
    "    labels = datalas[:, -1].astype(int)\n",
    "    del datala_list_end\n",
    "    gc.collect()\n",
    "    with h5py.File(folder_root + f'/{str(time_window)}{str(h_window)}_data.h5', 'w') as hf:\n",
    "        hf.create_dataset('data', data=data)\n",
    "\n",
    "    with h5py.File(folder_root + f'/{str(time_window)}{str(h_window)}_labels.h5', 'w') as hf:\n",
    "        hf.create_dataset('labels', data=labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human_action",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
