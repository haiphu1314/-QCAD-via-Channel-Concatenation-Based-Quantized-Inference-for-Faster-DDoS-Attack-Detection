{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "from scapy.all import rdpcap, IP, TCP, UDP, ICMP\n",
    "# from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "file_path_dos = f\"/mnt/d/dataset_pre/bot/dos_http.csv\"\n",
    "print(file_path_dos)\n",
    "df = pd.read_csv(file_path_dos, sep=';')\n",
    "# df = df[['saddr', 'sport', 'daddr', 'dport', 'proto', 'category']][df['proto']!='arp']\n",
    "# print(df['category'].unique())\n",
    "# df.to_csv(f\"/mnt/d/dataset_pre/bot/dos_udp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "from scapy.all import rdpcap, IP, TCP, UDP, ICMP\n",
    "# from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math \n",
    "def conv_proto(proto):\n",
    "    if proto == 'tcp':\n",
    "        return 6\n",
    "    elif proto == 'udp':\n",
    "        return 17\n",
    "    elif proto == 'icmp':\n",
    "        return 1    \n",
    "# 0\n",
    "def convert_to_int(value):\n",
    "    try:\n",
    "        if value == 'http':\n",
    "            return 80  # Giá trị cố định cho 'http'\n",
    "        elif isinstance(value, str) and value.startswith('0x'):\n",
    "            return int(value, 16)\n",
    "        elif value == 'login':\n",
    "            return 513\n",
    "        elif value == 'nut':\n",
    "            return 3487\n",
    "        elif value == 'xinetd':\n",
    "            return 10506\n",
    "        # elif math.isnan(value):\n",
    "        #     return 0\n",
    "        else:\n",
    "            print(value)\n",
    "            return int(float(value))\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Cannot convert {value} to int\")\n",
    "    \n",
    "def create_id(row):\n",
    "    if row[proto] in ['tcp', 'udp']:  # TCP or UDP\n",
    "        id_str = f\"{row[saddr]}:{int(row[sport])}->{row[daddr]}:{int(row[dport])}:{conv_proto(row[proto])}\"\n",
    "    elif row[proto] == 'icmp':  # ICMP\n",
    "        id_str = f\"{row[saddr]}->{row[daddr]}:1\"\n",
    "    return id_str\n",
    "\n",
    "NAME = 'key'\n",
    "\n",
    "saddr = 'saddr'\n",
    "sport = 'sport'\n",
    "daddr = 'daddr'\n",
    "dport = 'dport'\n",
    "proto = 'proto'\n",
    "\n",
    "label = 'category'\n",
    "nor_label = 'Normal'\n",
    "dos_label = 'DDoS'\n",
    "\n",
    "\n",
    "file_path_dos = f\"/mnt/d/dataset_pre/bot/{NAME}.csv\"\n",
    "print(file_path_dos)\n",
    "df = pd.read_csv(file_path_dos, sep=';')\n",
    "\n",
    "\n",
    "df = df[['saddr', 'sport', 'daddr', 'dport', 'proto', 'category']][df['proto']!='arp']\n",
    "df = df[df['proto']!='ipv6-icmp']\n",
    "df = df.dropna()\n",
    "df[dport] = [convert_to_int(val) for val in df[dport]]\n",
    "df[sport] = [convert_to_int(val) for val in df[sport]]\n",
    "\n",
    "# df.to_csv(f\"/mnt/d/dataset_pre/bot/{NAME}.csv\", index=False)\n",
    "\n",
    "nor = None\n",
    "dos = None\n",
    "\n",
    "nor = df[df[label] == nor_label]\n",
    "dos = df[(df['category'] == 'DoS') | (df['category'] == 'DDoS') | (df['category'] == 'Theft') | (df['category'] == 'Reconnaissance') ]\n",
    "\n",
    "nor_id = pd.DataFrame(nor.apply(create_id, axis=1).unique())\n",
    "dos_id = pd.DataFrame(dos.apply(create_id, axis=1).unique())\n",
    "\n",
    "nor_id.to_csv(f\"/mnt/d/dataset_pre/bot/csv/nor_{NAME}.csv\", index=False)\n",
    "dos_id.to_csv(f\"/mnt/d/dataset_pre/bot/csv/{NAME}.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Đường dẫn tới thư mục chứa các file cần đổi tên\n",
    "NAME = 'service'\n",
    "folder_path = f'/mnt/d/dataset_pre/bot/{NAME}/'\n",
    "\n",
    "# Hàm để đổi tên các file trong thư mục\n",
    "def rename_files_in_folder(folder_path):\n",
    "    # Duyệt qua từng file trong thư mục\n",
    "    i = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Đường dẫn đầy đủ của file cũ\n",
    "        old_file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Tạo tên mới cho file, ví dụ thêm tiền tố \"new_\"\n",
    "        new_filename =  f'{NAME}_' + str(i) +'.pcap'\n",
    "        # Đường dẫn đầy đủ của file mới\n",
    "        new_file_path = os.path.join(folder_path, new_filename)\n",
    "        \n",
    "        # Đổi tên file\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        print(f\"Đã đổi tên {old_file_path} thành {new_file_path}\")\n",
    "        i+=1\n",
    "# Gọi hàm để đổi tên các file trong thư mục\n",
    "rename_files_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, Manager, cpu_count\n",
    "import json\n",
    "import gc\n",
    "NAME = 'key'\n",
    "\n",
    "if 'os' in NAME:\n",
    "    LABEL = 3\n",
    "elif 'service' in NAME:\n",
    "    LABEL = 3\n",
    "elif 'exf' in NAME:\n",
    "    LABEL = 4\n",
    "elif 'key' in NAME:\n",
    "    LABEL = 4    \n",
    "\n",
    "if 'ddos' in NAME:\n",
    "    LABEL = 2\n",
    "else:\n",
    "    LABEL = 1\n",
    "\n",
    "\n",
    "\n",
    "folder_path = f\"/mnt/d/dataset_pre/bot/{NAME}\"\n",
    "TIMEOUT = 999\n",
    "cnt_normal = 0\n",
    "nor = {}\n",
    "dos = {}\n",
    "dos['ID'] = pd.read_csv(f\"/mnt/d/dataset_pre/bot/csv/{NAME}.csv\")\n",
    "nor['ID'] = pd.read_csv(f\"/mnt/d/dataset_pre/bot/csv/nor_{NAME}.csv\")\n",
    "# duplicate_ip = pd.Series(list(set(nor['ID']).intersection(set(dos['ID']))))\n",
    "a = None\n",
    "def read_pcap_with_tshark(file_path):\n",
    "    command = [\n",
    "        'tshark', '-r', file_path, '-T', 'fields',\n",
    "        '-e', 'frame.time_epoch',\n",
    "        '-e', 'ip.src', '-e', 'ip.dst',\n",
    "        '-e', 'ip.proto',\n",
    "        '-e', 'tcp.srcport', '-e', 'tcp.dstport', '-e', 'tcp.flags', '-e', 'tcp.ack',\n",
    "        '-e', 'udp.srcport', '-e', 'udp.dstport',\n",
    "        '-e', 'icmp.type',\n",
    "        '-e', 'frame.len', '-e', 'ip.flags', '-e', 'ip.ttl'\n",
    "    ]\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    for line in process.stdout:\n",
    "        yield line.decode().strip().split('\\t')\n",
    "\n",
    "for filename in range(0, 50):\n",
    "    file_path = folder_path + f'/{NAME}_' + str(filename) + '.pcap'\n",
    "    cnt_normal = 0\n",
    "    folroot = os.path.dirname(folder_path)\n",
    "    for folname in [folroot+'/data']:\n",
    "        if not os.path.exists(folname):\n",
    "                os.makedirs(folname)\n",
    "    if os.path.isfile(file_path):\n",
    "        \n",
    "        flows = {}\n",
    "        flows_cnt = {}\n",
    "        if not os.path.isfile(os.path.dirname(os.path.dirname(file_path))+'/data/'+file_path.split('/')[-1].replace('.pcap','.json')):\n",
    "            print(file_path)\n",
    "            for packet in tqdm(read_pcap_with_tshark(file_path)):\n",
    "                if len(packet) < 14:\n",
    "                    continue\n",
    "                timestamp, src_ip, dst_ip, protocol_numbers, tcp_sport, tcp_dport, tcp_flags, tcp_ack, udp_sport, udp_dport, icmp_type, pkt_len, ip_flags, ttl = packet\n",
    "\n",
    "                try:\n",
    "                    pkt_len = int(pkt_len, 16)\n",
    "                except:\n",
    "                    pkt_len = int(pkt_len.split(',')[0], 16)\n",
    "                try:\n",
    "                    ip_flags = int(ip_flags, 16)\n",
    "                except:\n",
    "                    ip_flags = int(ip_flags.split(',')[0], 16)    \n",
    "                try:\n",
    "                    ttl = int(ttl, 16)\n",
    "                except:\n",
    "                    ttl = int(ttl.split(',')[0], 16)\n",
    "                try:\n",
    "                    int(icmp_type,16)\n",
    "                except:\n",
    "                    icmp_type = icmp_type.split(',')[0]\n",
    "                proto_layer = None\n",
    "                flow_id_str = None\n",
    "                \n",
    "                for protocol_number in protocol_numbers.split(','):\n",
    "                    protocol_number = int(protocol_number)\n",
    "                    \n",
    "                    if tcp_sport and tcp_dport:\n",
    "                        proto_layer = 'TCP'\n",
    "                        flow_id_str = f\"{src_ip}:{tcp_sport}->{dst_ip}:{tcp_dport}:{protocol_number}\"\n",
    "                    elif udp_sport and udp_dport:\n",
    "                        proto_layer = 'UDP'\n",
    "                        flow_id_str = f\"{src_ip}:{udp_sport}->{dst_ip}:{udp_dport}:{protocol_number}\"\n",
    "                    elif icmp_type:\n",
    "                        proto_layer = 'ICMP'\n",
    "                        flow_id_str = f\"{src_ip}->{dst_ip}:{protocol_number}\"\n",
    "                    # print(flow_id_str)\n",
    "                    # break\n",
    "                    if flow_id_str:\n",
    "                        if 1==1:\n",
    "                            flow_id_hash = int(hashlib.md5(flow_id_str.encode()).hexdigest(), 32)\n",
    "                            if flow_id_hash not in flows_cnt:\n",
    "                                flows_cnt[flow_id_hash] = 0\n",
    "                            if proto_layer == 'TCP':\n",
    "                                try:\n",
    "                                    if int(tcp_flags, 16) & 0x02 and not int(tcp_flags, 16) & 0x10:  # SYN flag set and ACK flag not set\n",
    "                                        flows_cnt[flow_id_hash] += 1\n",
    "                                except:\n",
    "                                    aa=1\n",
    "                            flow_id = int(hashlib.md5((flow_id_str + ':' + str(flows_cnt[flow_id_hash])).encode()).hexdigest(), 32)\n",
    "                            if flow_id:\n",
    "                                sorted_flow_id = flow_id\n",
    "                                if sorted_flow_id not in flows:\n",
    "                                    flows[sorted_flow_id] = []\n",
    "                                packet_info = {\n",
    "                                    'id': flow_id_str,\n",
    "                                    'timestamp': float(timestamp),\n",
    "                                    'time': 0,\n",
    "                                    'proto': protocol_number,\n",
    "                                    'pkt_len': pkt_len,\n",
    "                                    'ip_flag': ip_flags if ip_flags else 0,\n",
    "                                    'ttl': ttl if ttl else 0,\n",
    "                                    'tcp_len': None,\n",
    "                                    'tcp_ack': int(tcp_ack, 16) if tcp_ack else 0,\n",
    "                                    'tcp_flag': int(tcp_flags, 16) if tcp_flags else 0,\n",
    "                                    'udp_len': None,\n",
    "                                    'icmp_type': int(icmp_type, 16) if icmp_type else 0,\n",
    "                                    'label': LABEL  # 0: Normal, 1: DoS, 2: DDoS\n",
    "                                }\n",
    "                                if proto_layer == 'TCP':\n",
    "                                    packet_info['tcp_len'] = int(tcp_sport, 16)\n",
    "                                elif proto_layer == 'UDP':\n",
    "                                    packet_info['udp_len'] = int(udp_sport, 16)\n",
    "                                \n",
    "                                if flows[sorted_flow_id]:\n",
    "                                    # a = flows[sorted_flow_id]['timestamp']\n",
    "                                    first_packet_time = flows[sorted_flow_id][0]['timestamp']\n",
    "                                    packet_info['time'] = float(timestamp) - first_packet_time\n",
    "                                if packet_info['time'] == 0:\n",
    "                                    if nor['ID'].isin([flow_id_str]).sum().values:\n",
    "                                        packet_info['label'] = 0\n",
    "                                        cnt_normal+=1\n",
    "                                flows[sorted_flow_id].append(packet_info)\n",
    "                # break\n",
    "            flow_arrays = list(flows.values())\n",
    "            for flow_id in flows:\n",
    "                for packet in flows[flow_id]:\n",
    "                    del packet['time']\n",
    "                    del packet['id']\n",
    "            print(f\"Số lượng các luồng Normal: {cnt_normal}\")\n",
    "            print(f\"Số lượng các luồng: {len(flow_arrays)}\")\n",
    "\n",
    "            output_file_path = os.path.dirname(os.path.dirname(file_path))+'/data/'+file_path.split('/')[-1].replace('.pcap','.json')\n",
    "            data = []\n",
    "            lb = []\n",
    "            time_window = 60\n",
    "            h_window = 32\n",
    "            e = time_window / h_window\n",
    "            \n",
    "            for flow_id in flows:\n",
    "                data_array = []\n",
    "                flow = flows[flow_id]\n",
    "                for h in range(h_window):\n",
    "                    data_array_t = []\n",
    "                    for record in flow:\n",
    "                        if (record['time'] < (h + 1) * e) and (record['time'] >= h * e):\n",
    "                            record_list = [\n",
    "                                record['proto'],\n",
    "                                record['pkt_len'],\n",
    "                                record['ip_flag'],\n",
    "                                record['ttl'],\n",
    "                                record['tcp_len'] if record['tcp_len'] is not None else 0,\n",
    "                                record['tcp_ack'] if record['tcp_ack'] is not None else 0,\n",
    "                                record['tcp_flag'] if record['tcp_flag'] is not None else 0,\n",
    "                                record['udp_len'] if record['udp_len'] is not None else 0,\n",
    "                                record['icmp_type'] if record['icmp_type'] is not None else 0,\n",
    "                            ]\n",
    "                            data_array_t.append(record_list)\n",
    "                    if len(data_array_t) == 0 or np.isnan(np.mean(data_array_t, axis=0)).any():\n",
    "                        data_array_t_mean = np.zeros((1, 9))\n",
    "                    else:\n",
    "                        data_array_t_mean = np.mean(data_array_t, axis=0)\n",
    "                    data_array.append(data_array_t_mean)\n",
    "                data.append(np.vstack(data_array))\n",
    "                lb.append(record['label'])\n",
    "            \n",
    "            np.save(output_file_path.replace('.json', '.npy'), data, allow_pickle=True)\n",
    "            np.save(output_file_path.replace('.json', 'labels.npy'), lb, allow_pickle=True)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py\n",
    "\n",
    "\n",
    "def unique_rows(matrix):\n",
    "    seen = set()\n",
    "    unique_indices = []\n",
    "    \n",
    "    for i, row in enumerate(matrix):\n",
    "        row_hash = hash(row.tobytes())\n",
    "        if row_hash not in seen:\n",
    "            seen.add(row_hash)\n",
    "            unique_indices.append(i)\n",
    "    unique_matrix = matrix[unique_indices]\n",
    "    return unique_matrix\n",
    "def round_to_n_significant_figures(tensor, n):\n",
    "    \"\"\"\n",
    "    Efficiently round each element in a NumPy array to n significant figures.\n",
    "    Handles edge cases to avoid NaN values.\n",
    "    \"\"\"\n",
    "    tensor = np.asarray(tensor)\n",
    "    # Handle zero separately to avoid division by zero\n",
    "    non_zero_mask = tensor != 0\n",
    "    abs_tensor = np.abs(tensor[non_zero_mask])\n",
    "    order_of_magnitude = np.floor(np.log10(abs_tensor))\n",
    "    scale = np.power(10, n - 1 - order_of_magnitude)\n",
    "    tensor[non_zero_mask] = np.round(tensor[non_zero_mask] * scale) / scale\n",
    "    return tensor\n",
    "for h_window in [64,32,16,8]:\n",
    "    datala_list = []\n",
    "    time_window = 1\n",
    "    folder_root = '/mnt/d/dataset_pre/bot'\n",
    "    folder_path = folder_root+f'/h{str(h_window)}b/time_window_{str(time_window)}s/data'\n",
    "    cnt = 0\n",
    "    cnt_file = 0\n",
    "    for i in range(1000):\n",
    "        # if i in range(0,50) or i in range(214,283):\n",
    "        if 1:\n",
    "            filename = str(i) + '.npy'\n",
    "            if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "                if(cnt%10==0 and cnt !=0):\n",
    "                    print(cnt)\n",
    "                if(cnt%100==0 and cnt !=0):\n",
    "                    datalas = np.vstack(datala_list)\n",
    "                    datalas = round_to_n_significant_figures(datalas,3) \n",
    "                    datala_t = unique_rows(datalas)\n",
    "                    data = datala_t[:, :-1].reshape(-1, h_window, 9).astype(np.float32)\n",
    "                    labels = datala_t[:, -1].astype(int)\n",
    "\n",
    "                    np.save(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_data.npy', data)\n",
    "                    np.save(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_labels.npy', labels)\n",
    "                    del datalas\n",
    "                    del datala_t\n",
    "                    del data\n",
    "                    del labels\n",
    "                    gc.collect()\n",
    "                    datala_list = []\n",
    "                    cnt_file+=1\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                data = np.load(file_path)\n",
    "                labels = np.load(file_path.replace('/data/', '/labels/'))\n",
    "                data = data.reshape(data.shape[0], -1).reshape(data.shape[0], -1)\n",
    "                labels = labels.reshape(-1, 1)\n",
    "\n",
    "                datala_e = np.hstack((data, labels))\n",
    "                datala_list.append(datala_e)\n",
    "                cnt+=1\n",
    "                del data\n",
    "                del labels\n",
    "                del datala_e\n",
    "                gc.collect()\n",
    "\n",
    "    datalas = np.vstack(datala_list)\n",
    "    datala_t = unique_rows(datalas)\n",
    "    data = datala_t[:, :-1].reshape(-1, h_window, 9).astype(np.float32)\n",
    "    labels = datala_t[:, -1].astype(int)\n",
    "\n",
    "    np.save(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_data.npy', data)\n",
    "    np.save(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_labels.npy', labels)\n",
    "    del data\n",
    "    del labels\n",
    "    del datala_t\n",
    "    del datalas\n",
    "    gc.collect()\n",
    "    datala_list = []\n",
    "    datala_list_end = []\n",
    "    tt = cnt_file+1\n",
    "    cnt_file = 0\n",
    "    for cnt_file in range(tt):\n",
    "        print(cnt_file)\n",
    "        if os.path.isfile(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_data.npy'):\n",
    "            data = np.load(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_data.npy')\n",
    "            labels = np.load(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_labels.npy')\n",
    "            os.remove(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_data.npy')\n",
    "            os.remove(folder_root+f'/{str(time_window)}{str(h_window)}_{str(cnt_file)}_labels.npy')\n",
    "\n",
    "            data = data.reshape(data.shape[0], -1).reshape(data.shape[0], -1)\n",
    "            labels = labels.reshape(-1, 1)\n",
    "            datala_e = np.hstack((data, labels))\n",
    "            datala_list.append(datala_e)\n",
    "            del data\n",
    "            del labels\n",
    "            del datala_e\n",
    "            gc.collect()\n",
    "\n",
    "    datalas = np.vstack(datala_list)\n",
    "    data = datalas[:, :-1].reshape(-1, h_window, 9).astype(np.float32)\n",
    "    labels = datalas[:, -1].astype(int)\n",
    "    del datala_list_end\n",
    "    gc.collect()\n",
    "    with h5py.File(folder_root + f'/{str(time_window)}{str(h_window)}_data.h5', 'w') as hf:\n",
    "        hf.create_dataset('data', data=data)\n",
    "\n",
    "    with h5py.File(folder_root + f'/{str(time_window)}{str(h_window)}_labels.h5', 'w') as hf:\n",
    "        hf.create_dataset('labels', data=labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human_action",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
